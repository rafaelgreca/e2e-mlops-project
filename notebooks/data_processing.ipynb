{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer\n",
    "from pprint import pprint\n",
    "import os\n",
    "import joblib\n",
    "import yaml\n",
    "import boto3\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def read_yaml_file(path, file):\n",
    "    # reading credentials files\n",
    "    with open(f\"{os.path.join(path, file)}\") as f:\n",
    "        try:\n",
    "            content = yaml.safe_load(f)\n",
    "        except yaml.YAMLError as e:\n",
    "            raise e\n",
    "    \n",
    "    return content\n",
    "\n",
    "CONFIG_PATH = os.path.join(\"..\", \"src\", \"config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_config = read_yaml_file(\n",
    "    path=CONFIG_PATH,\n",
    "    file=\"credentials.yaml\"\n",
    ")\n",
    "\n",
    "general_settings = read_yaml_file(\n",
    "    path=CONFIG_PATH,\n",
    "    file=\"settings.yaml\"\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "ARTIFACTS_OUTPUT_PATH = general_settings[\"ARTIFACTS_PATH\"]\n",
    "FEATURES_OUTPUT_PATH = general_settings[\"FEATURES_PATH\"]\n",
    "RAW_FILE_PATH = os.path.join(general_settings[\"DATA_PATH\"], general_settings[\"RAW_FILE_NAME\"])\n",
    "PROCESSED_RAW_FILE = \"Preprocessed_\" + general_settings[\"RAW_FILE_NAME\"]\n",
    "PROCESSED_RAW_FILE_PATH = os.path.join(general_settings[\"DATA_PATH\"], PROCESSED_RAW_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if credentials_config[\"S3\"] != \"YOUR_S3_BUCKET_URL\":\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=credentials_config[\"AWS_ACCESS_KEY\"],\n",
    "        aws_secret_access_key=credentials_config[\"AWS_SECRET_KEY\"]\n",
    "    )\n",
    "\n",
    "    # downloading the original file from the aws s3 bucket\n",
    "    if not os.path.exists(RAW_FILE_PATH):\n",
    "        s3.download_file(\n",
    "            credentials_config[\"S3\"],\n",
    "            general_settings[\"RAW_FILE_NAME\"],\n",
    "            RAW_FILE_PATH\n",
    "        )\n",
    "\n",
    "df = pd.read_csv(RAW_FILE_PATH, sep=\",\")\n",
    "df = df.drop(columns=[\"id\"])\n",
    "pprint(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(keep=\"first\")\n",
    "pprint(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Height Units to Centimeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Height\"] *= 100\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the upper and lower limits\n",
    "Q1 = df[\"Age\"].quantile(0.25)\n",
    "Q3 = df[\"Age\"].quantile(0.75)\n",
    "threshold = 3.5\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "pprint(f\"Dataset shape before removing the outliers: {df.shape}\")\n",
    "\n",
    "# removing the data samples that exceeds the upper or lower limits\n",
    "df = df[~((df[\"Age\"] >= (Q3 + threshold * IQR)) | (df[\"Age\"] <= (Q1 - threshold * IQR)))]\n",
    "pprint(f\"Dataset shape after removing the outliers: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Body Mass Index (BMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"BMI\"] = df[\"Weight\"] / (df[\"Height\"] ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Physical Activity Level (PAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"PAL\"] = df[\"FAF\"] - df[\"TUE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Body Surface Area (BSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bsa(\n",
    "    gender: str,\n",
    "    height: float,\n",
    "    weight: float\n",
    ") -> float:\n",
    "    # Schlich formula\n",
    "    if gender == \"Female\":\n",
    "        return 0.000975482 * (weight ** 0.46) * (height ** 1.08)\n",
    "\n",
    "    return 0.000579479 * (weight ** 0.38) * (height ** 1.24)\n",
    "\n",
    "df[\"BSA\"] = df.apply(lambda x: calculate_bsa(x[\"Gender\"], x[\"Height\"], x[\"Weight\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideal Body Weight (IBW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ibw(\n",
    "    gender: str,\n",
    "    height: float\n",
    ") -> float:\n",
    "    # B. J. Devine formula\n",
    "    if gender == \"Female\":\n",
    "        return 45.5 + 0.9 * (height - 152)\n",
    "\n",
    "    return 50 + 0.9 * (height - 152)\n",
    "\n",
    "df[\"IBW\"] = df.apply(lambda x: calculate_ibw(x[\"Gender\"], x[\"Height\"]), axis=1)\n",
    "df[\"diff_W_IBW\"] = df[\"Weight\"] - df[\"IBW\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basal Metabolic Rate (BMR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bmr(\n",
    "    age: int,\n",
    "    gender: str,\n",
    "    height: float,\n",
    "    weight: float\n",
    ") -> float:\n",
    "    s = -161 if gender == \"Female\" else 5\n",
    "    return (10 * weight) + (6.25 * height) - (5 * age) + s\n",
    "\n",
    "df[\"BMR\"] = df.apply(lambda x: calculate_bmr(x[\"Age\"], x[\"Gender\"], x[\"Height\"], x[\"Weight\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Daily Energy Expenditure (TDEE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tdee(\n",
    "    bmr: float,\n",
    "    activity: float\n",
    ") -> float:\n",
    "    if activity == 0:\n",
    "        return bmr * 1.2\n",
    "    elif activity < 1:\n",
    "        return bmr * 1.55\n",
    "    elif activity > 1 and activity <= 2:\n",
    "        return bmr * 1.725\n",
    "    else:\n",
    "        return bmr * 1.9\n",
    "\n",
    "df[\"TDEE\"] = df.apply(lambda x: calculate_tdee(x[\"BMR\"], x[\"FAF\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sufficient Water Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SWC\"] = df[\"CH2O\"] > ((df[\"Weight\"] / 2) * 0.0295735) # converting onces to liters\n",
    "df[\"SWC\"] = df[\"SWC\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is Sedentary? (IS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"IS\"] = df[\"FAF\"] <= 1\n",
    "df[\"IS\"] = df[\"IS\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Healthy Habits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_healthy_habits(\n",
    "    row: pd.DataFrame\n",
    ") -> float:\n",
    "    eat_healthy = -1 if (row[\"FCVC\"] * row[\"NCP\"]) < 3 else 1\n",
    "    is_sedentary = -1 if row[\"FAF\"] <= 1 else 1\n",
    "    is_smoker = -1 if row[\"SMOKE\"] == \"yes\" else 1\n",
    "    sufficient_water_consumption = -1 if (row[\"CH2O\"] < ((row[\"Weight\"] / 2) * 0.0295735)) else 1\n",
    "    drink_frequently = -1 if (row[\"CALC\"] == \"Always\" or row[\"CALC\"] == \"Frequently\") else 1\n",
    "    active_person = -1 if (row[\"TUE\"] - row[\"FAF\"]) > 0 else 1\n",
    "    is_overweight = -1 if (row[\"Height\"] - calculate_ibw(row[\"Age\"], row[\"Height\"])) > 0 else 1\n",
    "    \n",
    "    return eat_healthy + is_sedentary + is_smoker + sufficient_water_consumption + drink_frequently + active_person + is_overweight\n",
    "\n",
    "df[\"HH\"] = df.apply(lambda x: calculate_healthy_habits(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideal Number of Main Meals? (INMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"INMM\"] = df[\"NCP\"] == 3\n",
    "df[\"INMM\"] = df[\"INMM\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eat Vegetables Every Main Meal? (EVEMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"EVEMM\"] = (df[\"FCVC\"] >= df[\"NCP\"])\n",
    "df[\"EVEMM\"] = df[\"EVEMM\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 10\n",
    "ncols = 2\n",
    "\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "r, c = 0, 0\n",
    "\n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(15)\n",
    "\n",
    "# plotting numerical columns distributions\n",
    "numerical_columns = df.select_dtypes(exclude=\"object\").columns.tolist()\n",
    "\n",
    "for nc in numerical_columns:\n",
    "    print(nc)\n",
    "    if c == ncols:\n",
    "        c = 0\n",
    "        r += 1\n",
    "\n",
    "    sns.histplot(\n",
    "        data=df[nc],\n",
    "        ax=axs[r, c],\n",
    "        kde=True\n",
    "    )\n",
    "\n",
    "    axs[r, c].set_title(nc)\n",
    "    axs[r, c].set_xlabel(\"\")\n",
    "    axs[r, c].set_ylabel(\"\")\n",
    "\n",
    "    if c == 0:\n",
    "        axs[r, c].set_ylabel(\"Count\")\n",
    "\n",
    "    if (r == nrows - 1) or (r == nrows - 2 and c > 1):\n",
    "        axs[r, c].set_xlabel(\"Value\")\n",
    "\n",
    "    c += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming `Age` Column Into a Categorical Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, bins = pd.qcut(x=df[\"Age\"], q=4, retbins=True, labels=[\"q1\", \"q2\", \"q3\", \"q4\"])\n",
    "bins = np.concatenate(([-np.inf], bins[1:-1], [np.inf]))\n",
    "\n",
    "df[\"Age\"] = values\n",
    "df[\"Age\"] = df[\"Age\"].astype(\"object\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming `IS`, `SWC`, `EVEMM`, `INMM` into Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SWC\"] = df[\"SWC\"].astype(\"object\")\n",
    "df[\"IS\"] = df[\"IS\"].astype(\"object\")\n",
    "df[\"EVEMM\"] = df[\"EVEMM\"].astype(\"object\")\n",
    "df[\"INMM\"] = df[\"INMM\"].astype(\"object\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming `HH` Column Into a Categorical Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"HH\"] = df[\"HH\"].astype(int)\n",
    "df[\"HH\"] = pd.qcut(x=df[\"HH\"], q=3, labels=[\"bad\", \"ok\", \"good\"])\n",
    "df[\"HH\"] = df[\"HH\"].astype(\"object\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Data into Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"NObeyesdad\"])\n",
    "y = df[\"NObeyesdad\"].values\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.15,\n",
    "    stratify=y,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_valid = X_valid.reset_index(drop=True)\n",
    "\n",
    "pprint(f\"Train set shape: {X_train.shape} and {y_train.shape}\")\n",
    "pprint(f\"Validation set shape: {X_valid.shape} and {y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming the Numerical Columns (Log Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes(exclude=\"object\").columns.tolist()\n",
    "epsilon = 1e-10\n",
    "\n",
    "for nc in numerical_columns:\n",
    "    if not nc in [\"diff_W_IBW\", \"PAL\"]:\n",
    "        X_train[nc] = np.log(X_train[nc].values + epsilon)\n",
    "        X_valid[nc] = np.log(X_valid[nc].values + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling the Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(\"Training set skewness before scaling:\")\n",
    "pprint(X_train[numerical_columns].skew())\n",
    "print()\n",
    "pprint(\"Validation set skewness before scaling:\")\n",
    "pprint(X_valid[numerical_columns].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {}\n",
    "\n",
    "for nc in numerical_columns:\n",
    "    sc = StandardScaler()\n",
    "    X_train[nc] = sc.fit_transform(X_train[nc].values.reshape(-1, 1))\n",
    "    X_valid[nc] = sc.transform(X_valid[nc].values.reshape(-1, 1))\n",
    "    scalers[nc] = sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 7\n",
    "ncols = 2\n",
    "\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "r, c = 0, 0\n",
    "\n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(15)\n",
    "\n",
    "temp_train = X_train.copy()\n",
    "temp_train[\"set\"] = [\"train\"] * temp_train.shape[0]\n",
    "\n",
    "temp_valid = X_valid.copy()\n",
    "temp_valid[\"set\"] = [\"valid\"] * temp_valid.shape[0]\n",
    "\n",
    "temp = pd.concat(\n",
    "    [temp_train, temp_valid, temp_valid, temp_valid, temp_valid],\n",
    "    axis=0,\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "for nc in numerical_columns:\n",
    "    if c == ncols:\n",
    "        c = 0\n",
    "        r += 1\n",
    "\n",
    "    sns.histplot(\n",
    "        data=temp[[nc, \"set\"]],\n",
    "        x=nc,\n",
    "        hue=\"set\",\n",
    "        ax=axs[r, c],\n",
    "        kde=True\n",
    "    )\n",
    "\n",
    "    axs[r, c].set_title(nc)\n",
    "    axs[r, c].set_xlabel(\"\")\n",
    "    axs[r, c].set_ylabel(\"\")\n",
    "\n",
    "    if c == 0:\n",
    "        axs[r, c].set_ylabel(\"Count\")\n",
    "\n",
    "    if (r == nrows - 1) or (r == nrows - 2 and c > 1):\n",
    "        axs[r, c].set_xlabel(\"Value\")\n",
    "\n",
    "    c += 1\n",
    "\n",
    "del temp, temp_train, temp_valid\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(\"Training set skewness after scaling:\")\n",
    "pprint(X_train[numerical_columns].skew())\n",
    "print()\n",
    "pprint(\"Validation set skewness after scaling:\")\n",
    "pprint(X_valid[numerical_columns].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding the Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 7\n",
    "ncols = 2\n",
    "\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "r, c = 0, 0\n",
    "\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(10)\n",
    "\n",
    "# plotting categorical columns distributions\n",
    "categorical_columns = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "target_column = \"NObeyesdad\"\n",
    "categorical_columns.remove(target_column)\n",
    "\n",
    "for cc in categorical_columns:\n",
    "    if c == ncols:\n",
    "        c = 0\n",
    "        r += 1\n",
    "\n",
    "    temp = df[cc].value_counts().reset_index()\n",
    "    temp.columns = [\"Value\", \"Count\"]\n",
    "\n",
    "    sns.barplot(\n",
    "        data=temp,\n",
    "        y=\"Value\",\n",
    "        x=\"Count\",\n",
    "        palette=sns.dark_palette(\"#69d\", reverse=True, n_colors=temp.shape[0]),\n",
    "        ax=axs[r, c],\n",
    "        orient=\"h\"\n",
    "    )\n",
    "\n",
    "    for i in axs[r, c].containers:\n",
    "        axs[r, c].bar_label(i,)\n",
    "\n",
    "    axs[r, c].set_title(cc)\n",
    "    axs[r, c].set_xlabel(\"\")\n",
    "    axs[r, c].set_ylabel(\"\")\n",
    "\n",
    "    if c == 0:\n",
    "        axs[r, c].set_ylabel(\"Count\")\n",
    "\n",
    "    if (r == nrows - 1) or (r == nrows - 2 and c > 1):\n",
    "        axs[r, c].set_xlabel(\"Value\")\n",
    "\n",
    "    c += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = pd.DataFrame()\n",
    "new_valid_df = pd.DataFrame()\n",
    "\n",
    "encoders = {}\n",
    "\n",
    "for cc in categorical_columns:\n",
    "    ohe = OneHotEncoder(\n",
    "        drop=\"first\",\n",
    "        sparse_output=False,\n",
    "        handle_unknown=\"infrequent_if_exist\",\n",
    "        min_frequency=20,\n",
    "    )\n",
    "    \n",
    "    train_categorical_features = pd.DataFrame(\n",
    "        ohe.fit_transform(X_train[cc].values.reshape(-1, 1)),\n",
    "        columns=ohe.get_feature_names_out(),\n",
    "    )\n",
    "    train_categorical_features = train_categorical_features.add_prefix(cc + \"_\")\n",
    "    new_train_df = pd.concat([new_train_df, train_categorical_features], axis=1)\n",
    "\n",
    "    valid_categorical_features = pd.DataFrame(\n",
    "        ohe.transform(X_valid[cc].values.reshape(-1, 1)),\n",
    "        columns=ohe.get_feature_names_out(),\n",
    "    )\n",
    "    valid_categorical_features = valid_categorical_features.add_prefix(cc + \"_\")\n",
    "    new_valid_df = pd.concat([new_valid_df, valid_categorical_features], axis=1)\n",
    "\n",
    "    encoders[cc] = ohe\n",
    "\n",
    "new_train_df = pd.concat([new_train_df, X_train.drop(columns=categorical_columns)], axis=1)\n",
    "new_valid_df = pd.concat([new_valid_df, X_valid.drop(columns=categorical_columns)], axis=1)\n",
    "\n",
    "X_train = new_train_df.values.copy()\n",
    "X_valid = new_valid_df.values.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_label = LabelBinarizer(\n",
    "    sparse_output=False\n",
    ")\n",
    "\n",
    "original_y_train = y_train.copy()\n",
    "original_y_valid = y_valid.copy()\n",
    "\n",
    "y_train = ohe_label.fit_transform(y_train.reshape(-1, 1))\n",
    "y_valid = ohe_label.transform(y_valid.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(f\"Train set shape: {X_train.shape} and {y_train.shape}\")\n",
    "pprint(f\"Validation set shape: {X_valid.shape} and {y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the artifacts locally\n",
    "os.makedirs(ARTIFACTS_OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(FEATURES_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "joblib.dump(scalers, os.path.join(ARTIFACTS_OUTPUT_PATH, \"features_sc.pkl\"))\n",
    "joblib.dump(encoders, os.path.join(ARTIFACTS_OUTPUT_PATH, \"features_ohe.pkl\"))\n",
    "joblib.dump(ohe_label, os.path.join(ARTIFACTS_OUTPUT_PATH, \"label_ohe.pkl\"))\n",
    "joblib.dump(bins, os.path.join(ARTIFACTS_OUTPUT_PATH, \"qcut_bins.pkl\"))\n",
    "\n",
    "joblib.dump(X_train, os.path.join(FEATURES_OUTPUT_PATH, \"X_train.pkl\"))\n",
    "joblib.dump(y_train, os.path.join(FEATURES_OUTPUT_PATH, \"y_train.pkl\"))\n",
    "joblib.dump(X_valid, os.path.join(FEATURES_OUTPUT_PATH, \"X_valid.pkl\"))\n",
    "joblib.dump(y_valid, os.path.join(FEATURES_OUTPUT_PATH, \"y_valid.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the preprocessed dataset locally\n",
    "new_train_df[target_column] = original_y_train\n",
    "new_valid_df[target_column] = original_y_valid\n",
    "\n",
    "preprocessed_data = pd.concat([new_train_df, new_valid_df])\n",
    "preprocessed_data.to_csv(PROCESSED_RAW_FILE_PATH, index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sending the artifacts to the aws s3 bucket\n",
    "def upload_folder_s3(root_path: str):\n",
    "    try:\n",
    "        for path, _, files in os.walk(root_path):\n",
    "            directory_name = path.split(\"/\")[-2]\n",
    "            for file in files:\n",
    "                s3.upload_file(os.path.join(path, file), credentials_config[\"S3\"], os.path.join(directory_name, file))\n",
    "\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "\n",
    "if credentials_config[\"S3\"] != \"YOUR_S3_BUCKET_URL\":\n",
    "    \n",
    "    if os.path.exists(ARTIFACTS_OUTPUT_PATH):\n",
    "        upload_folder_s3(ARTIFACTS_OUTPUT_PATH)\n",
    "\n",
    "    if os.path.exists(FEATURES_OUTPUT_PATH):\n",
    "        upload_folder_s3(FEATURES_OUTPUT_PATH)\n",
    "\n",
    "    # sending preprocessed dataset saved locally to the aws s3 bucket\n",
    "    s3.upload_file(\n",
    "        PROCESSED_RAW_FILE_PATH,\n",
    "        credentials_config[\"S3\"],\n",
    "        PROCESSED_RAW_FILE\n",
    "    )\n",
    "\n",
    "    # removing downloaded dataset from local\n",
    "    os.remove(RAW_FILE_PATH)\n",
    "    os.remove(PROCESSED_RAW_FILE_PATH)\n",
    "\n",
    "    # removing the local artifacts and features\n",
    "    shutil.rmtree(ARTIFACTS_OUTPUT_PATH)\n",
    "    shutil.rmtree(FEATURES_OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e2e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
