{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer\n",
    "from pprint import pprint\n",
    "import os\n",
    "import joblib\n",
    "import yaml\n",
    "import boto3\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def read_yaml_file(path, file):\n",
    "    # reading credentials files\n",
    "    with open(f\"{os.path.join(path, file)}\") as f:\n",
    "        try:\n",
    "            content = yaml.safe_load(f)\n",
    "        except yaml.YAMLError as e:\n",
    "            raise e\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "CONFIG_PATH = os.path.join(\"src\", \"config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_config = read_yaml_file(path=CONFIG_PATH, file=\"credentials.yaml\")\n",
    "\n",
    "general_settings = read_yaml_file(path=CONFIG_PATH, file=\"settings.yaml\")\n",
    "\n",
    "SEED = 42\n",
    "ARTIFACTS_OUTPUT_PATH = general_settings[\"ARTIFACTS_PATH\"]\n",
    "FEATURES_OUTPUT_PATH = general_settings[\"FEATURES_PATH\"]\n",
    "RAW_FILE_PATH = os.path.join(\n",
    "    general_settings[\"DATA_PATH\"], general_settings[\"RAW_FILE_NAME\"]\n",
    ")\n",
    "PROCESSED_RAW_FILE = \"Preprocessed_\" + general_settings[\"RAW_FILE_NAME\"]\n",
    "PROCESSED_RAW_FILE_PATH = os.path.join(\n",
    "    general_settings[\"DATA_PATH\"], PROCESSED_RAW_FILE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if credentials_config[\"S3\"] != \"YOUR_S3_BUCKET_URL\":\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=credentials_config[\"AWS_ACCESS_KEY\"],\n",
    "        aws_secret_access_key=credentials_config[\"AWS_SECRET_KEY\"],\n",
    "    )\n",
    "\n",
    "    # downloading the original file from the aws s3 bucket\n",
    "    if not os.path.exists(RAW_FILE_PATH):\n",
    "        s3.download_file(\n",
    "            credentials_config[\"S3\"], general_settings[\"RAW_FILE_NAME\"], RAW_FILE_PATH\n",
    "        )\n",
    "\n",
    "df = pd.read_csv(RAW_FILE_PATH, sep=\",\")\n",
    "df = df.drop(columns=[\"id\"])\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(keep=\"first\")\n",
    "pprint(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Height Units to Centimeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Height\"] *= 100\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the upper and lower limits\n",
    "Q1 = df[\"Age\"].quantile(0.25)\n",
    "Q3 = df[\"Age\"].quantile(0.75)\n",
    "threshold = 3.5\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "pprint(f\"Dataset shape before removing the outliers: {df.shape}\")\n",
    "\n",
    "# removing the data samples that exceeds the upper or lower limits\n",
    "df = df[\n",
    "    ~((df[\"Age\"] >= (Q3 + threshold * IQR)) | (df[\"Age\"] <= (Q1 - threshold * IQR)))\n",
    "]\n",
    "pprint(f\"Dataset shape after removing the outliers: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Body Mass Index (BMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"BMI\"] = df[\"Weight\"] / (df[\"Height\"] ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Physical Activity Level (PAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"PAL\"] = df[\"FAF\"] - df[\"TUE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Body Surface Area (BSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bsa(gender: str, height: float, weight: float) -> float:\n",
    "    # Schlich formula\n",
    "    if gender == \"Female\":\n",
    "        return 0.000975482 * (weight**0.46) * (height**1.08)\n",
    "\n",
    "    return 0.000579479 * (weight**0.38) * (height**1.24)\n",
    "\n",
    "\n",
    "df[\"BSA\"] = df.apply(\n",
    "    lambda x: calculate_bsa(x[\"Gender\"], x[\"Height\"], x[\"Weight\"]), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideal Body Weight (IBW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ibw(gender: str, height: float) -> float:\n",
    "    # B. J. Devine formula\n",
    "    if gender == \"Female\":\n",
    "        return 45.5 + 0.9 * (height - 152)\n",
    "\n",
    "    return 50 + 0.9 * (height - 152)\n",
    "\n",
    "\n",
    "df[\"IBW\"] = df.apply(lambda x: calculate_ibw(x[\"Gender\"], x[\"Height\"]), axis=1)\n",
    "df[\"diff_W_IBW\"] = df[\"Weight\"] - df[\"IBW\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basal Metabolic Rate (BMR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bmr(age: int, gender: str, height: float, weight: float) -> float:\n",
    "    s = -161 if gender == \"Female\" else 5\n",
    "    return (10 * weight) + (6.25 * height) - (5 * age) + s\n",
    "\n",
    "\n",
    "df[\"BMR\"] = df.apply(\n",
    "    lambda x: calculate_bmr(x[\"Age\"], x[\"Gender\"], x[\"Height\"], x[\"Weight\"]), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Daily Energy Expenditure (TDEE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tdee(bmr: float, activity: float) -> float:\n",
    "    if activity == 0:\n",
    "        return bmr * 1.2\n",
    "    elif activity < 1:\n",
    "        return bmr * 1.55\n",
    "    elif activity > 1 and activity <= 2:\n",
    "        return bmr * 1.725\n",
    "    else:\n",
    "        return bmr * 1.9\n",
    "\n",
    "\n",
    "df[\"TDEE\"] = df.apply(lambda x: calculate_tdee(x[\"BMR\"], x[\"FAF\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sufficient Water Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SWC\"] = df[\"CH2O\"] > ((df[\"Weight\"] / 2) * 0.0295735)  # converting onces to liters\n",
    "df[\"SWC\"] = df[\"SWC\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is Sedentary? (IS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"IS\"] = df[\"FAF\"] <= 1\n",
    "df[\"IS\"] = df[\"IS\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Healthy Habits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_healthy_habits(row: pd.DataFrame) -> float:\n",
    "    eat_healthy = -1 if (row[\"FCVC\"] * row[\"NCP\"]) < 3 else 1\n",
    "    is_sedentary = -1 if row[\"FAF\"] <= 1 else 1\n",
    "    is_smoker = -1 if row[\"SMOKE\"] == \"yes\" else 1\n",
    "    sufficient_water_consumption = (\n",
    "        -1 if (row[\"CH2O\"] < ((row[\"Weight\"] / 2) * 0.0295735)) else 1\n",
    "    )\n",
    "    drink_frequently = (\n",
    "        -1 if (row[\"CALC\"] == \"Always\" or row[\"CALC\"] == \"Frequently\") else 1\n",
    "    )\n",
    "    active_person = -1 if (row[\"TUE\"] - row[\"FAF\"]) > 0 else 1\n",
    "    is_overweight = (\n",
    "        -1 if (row[\"Height\"] - calculate_ibw(row[\"Age\"], row[\"Height\"])) > 0 else 1\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        eat_healthy\n",
    "        + is_sedentary\n",
    "        + is_smoker\n",
    "        + sufficient_water_consumption\n",
    "        + drink_frequently\n",
    "        + active_person\n",
    "        + is_overweight\n",
    "    )\n",
    "\n",
    "\n",
    "df[\"HH\"] = df.apply(lambda x: calculate_healthy_habits(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideal Number of Main Meals? (INMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"INMM\"] = df[\"NCP\"] == 3\n",
    "df[\"INMM\"] = df[\"INMM\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eat Vegetables Every Main Meal? (EVEMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"EVEMM\"] = df[\"FCVC\"] >= df[\"NCP\"]\n",
    "df[\"EVEMM\"] = df[\"EVEMM\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 10\n",
    "ncols = 2\n",
    "\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "r, c = 0, 0\n",
    "\n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(15)\n",
    "\n",
    "# plotting numerical columns distributions\n",
    "numerical_columns = df.select_dtypes(exclude=\"object\").columns.tolist()\n",
    "\n",
    "for nc in numerical_columns:\n",
    "    print(nc)\n",
    "    if c == ncols:\n",
    "        c = 0\n",
    "        r += 1\n",
    "\n",
    "    sns.histplot(data=df[nc], ax=axs[r, c], kde=True)\n",
    "\n",
    "    axs[r, c].set_title(nc)\n",
    "    axs[r, c].set_xlabel(\"\")\n",
    "    axs[r, c].set_ylabel(\"\")\n",
    "\n",
    "    if c == 0:\n",
    "        axs[r, c].set_ylabel(\"Count\")\n",
    "\n",
    "    if (r == nrows - 1) or (r == nrows - 2 and c > 1):\n",
    "        axs[r, c].set_xlabel(\"Value\")\n",
    "\n",
    "    c += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming `Age` Column Into a Categorical Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, bins = pd.qcut(x=df[\"Age\"], q=4, retbins=True, labels=[\"q1\", \"q2\", \"q3\", \"q4\"])\n",
    "bins = np.concatenate(([-np.inf], bins[1:-1], [np.inf]))\n",
    "\n",
    "df[\"Age\"] = values\n",
    "df[\"Age\"] = df[\"Age\"].astype(\"object\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming `IS`, `SWC`, `EVEMM`, `INMM` into Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SWC\"] = df[\"SWC\"].astype(\"object\")\n",
    "df[\"IS\"] = df[\"IS\"].astype(\"object\")\n",
    "df[\"EVEMM\"] = df[\"EVEMM\"].astype(\"object\")\n",
    "df[\"INMM\"] = df[\"INMM\"].astype(\"object\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming `HH` Column Into a Categorical Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"HH\"] = df[\"HH\"].astype(int)\n",
    "df[\"HH\"] = pd.qcut(x=df[\"HH\"], q=3, labels=[\"bad\", \"ok\", \"good\"])\n",
    "df[\"HH\"] = df[\"HH\"].astype(\"object\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Data into Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"NObeyesdad\"])\n",
    "y = df[\"NObeyesdad\"].values\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.15, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_valid = X_valid.reset_index(drop=True)\n",
    "\n",
    "pprint(f\"Train set shape: {X_train.shape} and {y_train.shape}\")\n",
    "pprint(f\"Validation set shape: {X_valid.shape} and {y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming the Numerical Columns (Log Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes(exclude=\"object\").columns.tolist()\n",
    "epsilon = 1e-10\n",
    "\n",
    "for nc in numerical_columns:\n",
    "    if not nc in [\"diff_W_IBW\", \"PAL\"]:\n",
    "        X_train[nc] = np.log(X_train[nc].values + epsilon)\n",
    "        X_valid[nc] = np.log(X_valid[nc].values + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling the Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(\"Training set skewness before scaling:\")\n",
    "pprint(X_train[numerical_columns].skew())\n",
    "print()\n",
    "pprint(\"Validation set skewness before scaling:\")\n",
    "pprint(X_valid[numerical_columns].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {}\n",
    "\n",
    "for nc in numerical_columns:\n",
    "    sc = StandardScaler()\n",
    "    X_train[nc] = sc.fit_transform(X_train[nc].values.reshape(-1, 1))\n",
    "    X_valid[nc] = sc.transform(X_valid[nc].values.reshape(-1, 1))\n",
    "    scalers[nc] = sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 7\n",
    "ncols = 2\n",
    "\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "r, c = 0, 0\n",
    "\n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(15)\n",
    "\n",
    "temp_train = X_train.copy()\n",
    "temp_train[\"set\"] = [\"train\"] * temp_train.shape[0]\n",
    "\n",
    "temp_valid = X_valid.copy()\n",
    "temp_valid[\"set\"] = [\"valid\"] * temp_valid.shape[0]\n",
    "\n",
    "temp = pd.concat(\n",
    "    [temp_train, temp_valid, temp_valid, temp_valid, temp_valid],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "for nc in numerical_columns:\n",
    "    if c == ncols:\n",
    "        c = 0\n",
    "        r += 1\n",
    "\n",
    "    sns.histplot(data=temp[[nc, \"set\"]], x=nc, hue=\"set\", ax=axs[r, c], kde=True)\n",
    "\n",
    "    axs[r, c].set_title(nc)\n",
    "    axs[r, c].set_xlabel(\"\")\n",
    "    axs[r, c].set_ylabel(\"\")\n",
    "\n",
    "    if c == 0:\n",
    "        axs[r, c].set_ylabel(\"Count\")\n",
    "\n",
    "    if (r == nrows - 1) or (r == nrows - 2 and c > 1):\n",
    "        axs[r, c].set_xlabel(\"Value\")\n",
    "\n",
    "    c += 1\n",
    "\n",
    "del temp, temp_train, temp_valid\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(\"Training set skewness after scaling:\")\n",
    "pprint(X_train[numerical_columns].skew())\n",
    "print()\n",
    "pprint(\"Validation set skewness after scaling:\")\n",
    "pprint(X_valid[numerical_columns].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding the Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 7\n",
    "ncols = 2\n",
    "\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "r, c = 0, 0\n",
    "\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(10)\n",
    "\n",
    "# plotting categorical columns distributions\n",
    "categorical_columns = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "target_column = \"NObeyesdad\"\n",
    "categorical_columns.remove(target_column)\n",
    "\n",
    "for cc in categorical_columns:\n",
    "    if c == ncols:\n",
    "        c = 0\n",
    "        r += 1\n",
    "\n",
    "    temp = df[cc].value_counts().reset_index()\n",
    "    temp.columns = [\"Value\", \"Count\"]\n",
    "\n",
    "    sns.barplot(\n",
    "        data=temp,\n",
    "        y=\"Value\",\n",
    "        x=\"Count\",\n",
    "        palette=sns.dark_palette(\"#69d\", reverse=True, n_colors=temp.shape[0]),\n",
    "        ax=axs[r, c],\n",
    "        orient=\"h\",\n",
    "    )\n",
    "\n",
    "    for i in axs[r, c].containers:\n",
    "        axs[r, c].bar_label(\n",
    "            i,\n",
    "        )\n",
    "\n",
    "    axs[r, c].set_title(cc)\n",
    "    axs[r, c].set_xlabel(\"\")\n",
    "    axs[r, c].set_ylabel(\"\")\n",
    "\n",
    "    if c == 0:\n",
    "        axs[r, c].set_ylabel(\"Count\")\n",
    "\n",
    "    if (r == nrows - 1) or (r == nrows - 2 and c > 1):\n",
    "        axs[r, c].set_xlabel(\"Value\")\n",
    "\n",
    "    c += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = pd.DataFrame()\n",
    "new_valid_df = pd.DataFrame()\n",
    "\n",
    "encoders = {}\n",
    "\n",
    "for cc in categorical_columns:\n",
    "    ohe = OneHotEncoder(\n",
    "        drop=\"first\",\n",
    "        sparse_output=False,\n",
    "        handle_unknown=\"infrequent_if_exist\",\n",
    "        min_frequency=20,\n",
    "    )\n",
    "\n",
    "    train_categorical_features = pd.DataFrame(\n",
    "        ohe.fit_transform(X_train[cc].values.reshape(-1, 1)),\n",
    "        columns=ohe.get_feature_names_out(),\n",
    "    )\n",
    "    train_categorical_features = train_categorical_features.add_prefix(cc + \"_\")\n",
    "    new_train_df = pd.concat([new_train_df, train_categorical_features], axis=1)\n",
    "\n",
    "    valid_categorical_features = pd.DataFrame(\n",
    "        ohe.transform(X_valid[cc].values.reshape(-1, 1)),\n",
    "        columns=ohe.get_feature_names_out(),\n",
    "    )\n",
    "    valid_categorical_features = valid_categorical_features.add_prefix(cc + \"_\")\n",
    "    new_valid_df = pd.concat([new_valid_df, valid_categorical_features], axis=1)\n",
    "\n",
    "    encoders[cc] = ohe\n",
    "\n",
    "new_train_df = pd.concat(\n",
    "    [new_train_df, X_train.drop(columns=categorical_columns)], axis=1\n",
    ")\n",
    "new_valid_df = pd.concat(\n",
    "    [new_valid_df, X_valid.drop(columns=categorical_columns)], axis=1\n",
    ")\n",
    "\n",
    "X_train = new_train_df.values.copy()\n",
    "X_valid = new_valid_df.values.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_label = LabelBinarizer(sparse_output=False)\n",
    "\n",
    "original_y_train = y_train.copy()\n",
    "original_y_valid = y_valid.copy()\n",
    "\n",
    "y_train = ohe_label.fit_transform(y_train.reshape(-1, 1))\n",
    "y_valid = ohe_label.transform(y_valid.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(f\"Train set shape: {X_train.shape} and {y_train.shape}\")\n",
    "pprint(f\"Validation set shape: {X_valid.shape} and {y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the artifacts locally\n",
    "os.makedirs(ARTIFACTS_OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(FEATURES_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "joblib.dump(scalers, os.path.join(ARTIFACTS_OUTPUT_PATH, \"features_sc.pkl\"))\n",
    "joblib.dump(encoders, os.path.join(ARTIFACTS_OUTPUT_PATH, \"features_ohe.pkl\"))\n",
    "joblib.dump(ohe_label, os.path.join(ARTIFACTS_OUTPUT_PATH, \"label_ohe.pkl\"))\n",
    "joblib.dump(bins, os.path.join(ARTIFACTS_OUTPUT_PATH, \"qcut_bins.pkl\"))\n",
    "\n",
    "joblib.dump(X_train, os.path.join(FEATURES_OUTPUT_PATH, \"X_train.pkl\"))\n",
    "joblib.dump(y_train, os.path.join(FEATURES_OUTPUT_PATH, \"y_train.pkl\"))\n",
    "joblib.dump(X_valid, os.path.join(FEATURES_OUTPUT_PATH, \"X_valid.pkl\"))\n",
    "joblib.dump(y_valid, os.path.join(FEATURES_OUTPUT_PATH, \"y_valid.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the preprocessed dataset locally\n",
    "new_train_df[target_column] = original_y_train\n",
    "new_valid_df[target_column] = original_y_valid\n",
    "\n",
    "preprocessed_data = pd.concat([new_train_df, new_valid_df])\n",
    "preprocessed_data.to_csv(PROCESSED_RAW_FILE_PATH, index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sending the artifacts to the aws s3 bucket\n",
    "def upload_folder_s3(root_path: str):\n",
    "    try:\n",
    "        for path, _, files in os.walk(root_path):\n",
    "            directory_name = path.split(\"/\")[-2]\n",
    "            for file in files:\n",
    "                s3.upload_file(\n",
    "                    os.path.join(path, file),\n",
    "                    credentials_config[\"S3\"],\n",
    "                    os.path.join(directory_name, file),\n",
    "                )\n",
    "\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "\n",
    "\n",
    "if credentials_config[\"S3\"] != \"YOUR_S3_BUCKET_URL\":\n",
    "\n",
    "    if os.path.exists(ARTIFACTS_OUTPUT_PATH):\n",
    "        upload_folder_s3(ARTIFACTS_OUTPUT_PATH)\n",
    "\n",
    "    if os.path.exists(FEATURES_OUTPUT_PATH):\n",
    "        upload_folder_s3(FEATURES_OUTPUT_PATH)\n",
    "\n",
    "    # sending preprocessed dataset saved locally to the aws s3 bucket\n",
    "    s3.upload_file(\n",
    "        PROCESSED_RAW_FILE_PATH, credentials_config[\"S3\"], PROCESSED_RAW_FILE\n",
    "    )\n",
    "\n",
    "    # removing downloaded dataset from local\n",
    "    os.remove(RAW_FILE_PATH)\n",
    "    os.remove(PROCESSED_RAW_FILE_PATH)\n",
    "\n",
    "    # removing the local artifacts and features\n",
    "    shutil.rmtree(ARTIFACTS_OUTPUT_PATH)\n",
    "    shutil.rmtree(FEATURES_OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
